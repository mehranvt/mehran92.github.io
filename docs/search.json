[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mehran92.github.io",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nMehran Islam\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nMehran Islam\n\n\n\n\n\n\n  \n\n\n\n\nOutlier\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nMehran Islam\n\n\n\n\n\n\n  \n\n\n\n\nProbability and random variable\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nMehran Islam\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nMehran Islam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification Post\nImport the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nRead data from file and identify null values\n\ndata=pd.read_csv('athlete_events.csv', index_col=\"ID\")\n\n{data.apply(pd.isnull).sum()/data.shape[0]}\n\nathletes= data[[\"Team\",\"Sex\", \"Season\",\"Sport\",\"Medal\"]].copy()\n\n#converting into column\nathletes.columns = [\"country\",\"sex\",\"season\", \"sport\",\"medal\"]\n\nathletes\n\n\n\n\n\n\n\n\ncountry\nsex\nseason\nsport\nmedal\n\n\nID\n\n\n\n\n\n\n\n\n\n1\nChina\nM\nSummer\nBasketball\nNaN\n\n\n2\nChina\nM\nSummer\nJudo\nNaN\n\n\n3\nDenmark\nM\nSummer\nFootball\nNaN\n\n\n4\nDenmark/Sweden\nM\nSummer\nTug-Of-War\nGold\n\n\n5\nNetherlands\nF\nWinter\nSpeed Skating\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n135569\nPoland-1\nM\nWinter\nLuge\nNaN\n\n\n135570\nPoland\nM\nWinter\nSki Jumping\nNaN\n\n\n135570\nPoland\nM\nWinter\nSki Jumping\nNaN\n\n\n135571\nPoland\nM\nWinter\nBobsleigh\nNaN\n\n\n135571\nPoland\nM\nWinter\nBobsleigh\nNaN\n\n\n\n\n271116 rows × 5 columns\n\n\n\n\nathletes.apply(pd.isnull).sum()/athletes.shape[0]\n\ncountry    0.000000\nsex        0.000000\nseason     0.000000\nsport      0.000000\nmedal      0.853262\ndtype: float64\n\n\n\nathletes = athletes[athletes['medal'].isin(['Gold', 'Silver', 'Bronze'])].dropna(subset=['medal'])\n\n\nathletes\n\n\n\n\n\n\n\n\ncountry\nsex\nseason\nsport\nmedal\n\n\nID\n\n\n\n\n\n\n\n\n\n4\nDenmark/Sweden\nM\nSummer\nTug-Of-War\nGold\n\n\n15\nFinland\nM\nSummer\nSwimming\nBronze\n\n\n15\nFinland\nM\nSummer\nSwimming\nBronze\n\n\n16\nFinland\nM\nWinter\nIce Hockey\nBronze\n\n\n17\nFinland\nM\nSummer\nGymnastics\nBronze\n\n\n...\n...\n...\n...\n...\n...\n\n\n135553\nSoviet Union\nF\nSummer\nAthletics\nSilver\n\n\n135553\nSoviet Union\nF\nSummer\nAthletics\nBronze\n\n\n135554\nPoland\nM\nSummer\nFencing\nBronze\n\n\n135563\nRussia\nF\nSummer\nAthletics\nBronze\n\n\n135563\nRussia\nF\nSummer\nAthletics\nSilver\n\n\n\n\n39783 rows × 5 columns\n\n\n\n\n# here we see a high percentage of null values in medal because only some of the athletes win the medal\n\n# Create a new DataFrame with the converted 'target' column\nnew_athletes = athletes.copy()  # Make a copy to avoid modifying the original DataFrame\n\n# Convert the 'medal' column to 'target' based on the medal values\nnew_athletes['target'] = new_athletes['medal'].apply(lambda x: 'gold' if x =='Gold' else 'no gold')\n\n\nnew_athletes\n\n\n\n\n\n\n\n\ncountry\nsex\nseason\nsport\nmedal\ntarget\n\n\nID\n\n\n\n\n\n\n\n\n\n\n4\nDenmark/Sweden\nM\nSummer\nTug-Of-War\nGold\ngold\n\n\n15\nFinland\nM\nSummer\nSwimming\nBronze\nno gold\n\n\n15\nFinland\nM\nSummer\nSwimming\nBronze\nno gold\n\n\n16\nFinland\nM\nWinter\nIce Hockey\nBronze\nno gold\n\n\n17\nFinland\nM\nSummer\nGymnastics\nBronze\nno gold\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n135553\nSoviet Union\nF\nSummer\nAthletics\nSilver\nno gold\n\n\n135553\nSoviet Union\nF\nSummer\nAthletics\nBronze\nno gold\n\n\n135554\nPoland\nM\nSummer\nFencing\nBronze\nno gold\n\n\n135563\nRussia\nF\nSummer\nAthletics\nBronze\nno gold\n\n\n135563\nRussia\nF\nSummer\nAthletics\nSilver\nno gold\n\n\n\n\n39783 rows × 6 columns\n\n\n\n\n#I just want to include four sports in which US generally do good\n\nselected_sports = ['Swimming']\n\nrecent_athletes = new_athletes[new_athletes['sport'].isin(selected_sports)]\n\n\nrecent_athletes.apply(pd.isnull).sum()\nrecent_athletes\n\n\n\n\n\n\n\n\ncountry\nsex\nseason\nsport\nmedal\ntarget\n\n\nID\n\n\n\n\n\n\n\n\n\n\n15\nFinland\nM\nSummer\nSwimming\nBronze\nno gold\n\n\n15\nFinland\nM\nSummer\nSwimming\nBronze\nno gold\n\n\n100\nHungary\nM\nSummer\nSwimming\nBronze\nno gold\n\n\n259\nCanada\nF\nSummer\nSwimming\nBronze\nno gold\n\n\n424\nSouth Africa\nF\nSummer\nSwimming\nBronze\nno gold\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n135302\nWest Germany\nF\nSummer\nSwimming\nBronze\nno gold\n\n\n135396\nNetherlands\nM\nSummer\nSwimming\nBronze\nno gold\n\n\n135415\nSoviet Union\nM\nSummer\nSwimming\nGold\ngold\n\n\n135489\nRussia\nF\nSummer\nSwimming\nSilver\nno gold\n\n\n135522\nNetherlands\nM\nSummer\nSwimming\nSilver\nno gold\n\n\n\n\n3048 rows × 6 columns\n\n\n\n\n#lets drop silver medals and gold medals because we are only interested in Gold medals\n\n#athletes = athletes[athletes['medal'] == 'Gold'].dropna(subset=['medal'])\n\n\n# Create a new DataFrame with the converted columns\nplayers = recent_athletes.copy()  # Make a copy to avoid modifying the original DataFrame\n\n# Convert the 'country' column to numerical values\nplayers['country'] = (recent_athletes['country'] == 'United States').astype(int)\n\n# Convert the 'sex' column to numerical values\nplayers['sex'] = (recent_athletes['sex'] == 'M').astype(int)\n\n\n# Convert the 'season' column to numerical values\nplayers['season'] = (recent_athletes['season'] == 'Summer').astype(int)\n\n# Convert the 'sport' column to numerical values\nplayers['sport'] = (recent_athletes['sport'] == 'Swimming').astype(int)\n\n\n#since we are only interested in swimming data let's drop others\n\n#athletes = athletes[athletes['sport'] == 'Swimming'].dropna(subset=['sport'])\n\n\nplayers\n\n\n\n\n\n\n\n\ncountry\nsex\nseason\nsport\nmedal\ntarget\n\n\nID\n\n\n\n\n\n\n\n\n\n\n15\n0\n1\n1\n1\nBronze\nno gold\n\n\n15\n0\n1\n1\n1\nBronze\nno gold\n\n\n100\n0\n1\n1\n1\nBronze\nno gold\n\n\n259\n0\n0\n1\n1\nBronze\nno gold\n\n\n424\n0\n0\n1\n1\nBronze\nno gold\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n135302\n0\n0\n1\n1\nBronze\nno gold\n\n\n135396\n0\n1\n1\n1\nBronze\nno gold\n\n\n135415\n0\n1\n1\n1\nGold\ngold\n\n\n135489\n0\n0\n1\n1\nSilver\nno gold\n\n\n135522\n0\n1\n1\n1\nSilver\nno gold\n\n\n\n\n3048 rows × 6 columns\n\n\n\n\n# Split the data into features (X) and the target variable (y)\n# to preapre data for ML ready\n\nfrom sklearn.model_selection import train_test_split\nX = players[['country','sport', 'sex']]\ny = players['target']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nplayers\n\n\n\n\n\n\n\n\ncountry\nsex\nseason\nsport\nmedal\ntarget\n\n\nID\n\n\n\n\n\n\n\n\n\n\n15\n0\n1\n1\n1\nBronze\nno gold\n\n\n15\n0\n1\n1\n1\nBronze\nno gold\n\n\n100\n0\n1\n1\n1\nBronze\nno gold\n\n\n259\n0\n0\n1\n1\nBronze\nno gold\n\n\n424\n0\n0\n1\n1\nBronze\nno gold\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n135302\n0\n0\n1\n1\nBronze\nno gold\n\n\n135396\n0\n1\n1\n1\nBronze\nno gold\n\n\n135415\n0\n1\n1\n1\nGold\ngold\n\n\n135489\n0\n0\n1\n1\nSilver\nno gold\n\n\n135522\n0\n1\n1\n1\nSilver\nno gold\n\n\n\n\n3048 rows × 6 columns\n\n\n\n\nX_test.shape\n\n(1524, 3)\n\n\n\nsns.countplot(data=players, x='target')\nplt.title('Distribution of Target')\nplt.show()\n\n\n\n\n\n# to train the naive model as out target is to use naive bayes model\n\n# we use gaussian naive bayes\n\nfrom sklearn.naive_bayes import GaussianNB\n\n# Create a Gaussian Naive Bayes model\nnb_model = GaussianNB()\n\n# Train the model on the training data\nnb_model.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n# now evaluating the model \n\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Predict on the test data\ny_pred = nb_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred)\nprint(\"Classification Report:\\n\", report)\n\nAccuracy: 0.7099737532808399\nClassification Report:\n               precision    recall  f1-score   support\n\n        gold       0.59      0.58      0.58       537\n     no gold       0.77      0.78      0.78       987\n\n    accuracy                           0.71      1524\n   macro avg       0.68      0.68      0.68      1524\nweighted avg       0.71      0.71      0.71      1524\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\n\n# Assuming you have already trained your Naive Bayes model (nb_model) and split your data into training (X_train, y_train) and testing (X_test, y_test) sets.\n\n# Example data (replace with your actual data)\n# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n# nb_model = MultinomialNB()\n# nb_model.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = nb_model.predict(X_test)\n\n# Create a confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Create a heatmap using seaborn\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n\n\n\n\n# Assuming you have the classification report stored in the 'report' variable\nreport = classification_report(y_test, y_pred, output_dict=True)\n\n# Convert the classification report to a DataFrame for easier plotting\nplayers_report = pd.DataFrame(report).transpose()\n\n# Create a horizontal bar chart using Seaborn\nplt.figure(figsize=(8, 4))\nsns.set(style=\"whitegrid\")\nsns.set_palette(\"pastel\")\nax = sns.barplot(x=players_report['f1-score'], y=players_report.index, orient=\"h\")\nax.set(xlabel='F1-Score', ylabel='Metric')\nplt.title('Classification Report Metrics')\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\n\n# Assuming you have already trained your Naive Bayes model (nb_model) and split your data into training (X_train, y_train) and testing (X_test, y_test) sets.\n\n# Example data (replace with your actual data)\n# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n# nb_model = MultinomialNB()\n# nb_model.fit(X_train, y_train)\n\n# Predict probabilities on the test data\ny_probs = nb_model.predict_proba(X_test)[:, 1]\n\n# Binarize the target variable\ny_test_binary = label_binarize(y_test, classes=np.unique(y_test))\n\n# Calculate precision-recall curve\nprecision, recall, _ = precision_recall_curve(y_test_binary, y_probs)\n\n# Calculate ROC curve\nfpr, tpr, _ = roc_curve(y_test_binary, y_probs)\n\n# Calculate area under the curves (AUC)\npr_auc = auc(recall, precision)\nroc_auc = auc(fpr, tpr)\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='darkorange', lw=2, label=f'PR Curve (AUC = {pr_auc:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='best')\nplt.show()\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkblue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='best')\nplt.show()"
  },
  {
    "objectID": "posts/Outlier/index.html",
    "href": "posts/Outlier/index.html",
    "title": "Outlier",
    "section": "",
    "text": "Sir Don Bradman and Sachin Tendulkar are widely considered as two of the most prolific batsmen of the cricketing world. Among hundreds and thousands of professional cricketers, they have garnered special god-like status among the cricket fans. Here, we will see from the plots and codes that they truly are something out of the ordinary.\nImporting libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nRead the data\n\ndata = pd.read_csv('Cricket_data.txt', sep='\\t')\n\n# Display the first few rows of the data\ndata\n\n\n\n\n\n\n\n\nPlayer\nSpan\nMat\nInn\nNO\nRuns\nHS\nAvg\n100\n50\n0\nPlayer Profile\n\n\n\n\n0\nSR Tendulkar (INDIA)\n1989-2013\n200\n329\n33\n15921\n248*\n53.78\n51\n68\n14\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n1\nRT Ponting (AUS)\n1995-2012\n168\n287\n29\n13378\n257\n51.85\n41\n62\n17\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n2\nJH Kallis (ICC/SA)\n1995-2013\n166\n280\n40\n13289\n224\n55.37\n45\n58\n16\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n3\nR Dravid (ICC/INDIA)\n1996-2012\n164\n286\n32\n13288\n270\n52.31\n36\n63\n8\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n4\nAN Cook (ENG)\n2006-2018\n161\n291\n16\n12472\n294\n45.35\n33\n57\n9\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2996\nCA Snedden (NZ)\n1947-1947\n1\n-\n-\n-\n-\n-\n-\n-\n-\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n2997\nVN Swamy (INDIA)\n1955-1955\n1\n-\n-\n-\n-\n-\n-\n-\n-\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n2998\nUsman Shinwari (PAK)\n2019-2019\n1\n-\n-\n-\n-\n-\n-\n-\n-\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n2999\nCM Willoughby (SA)\n2003-2003\n2\n-\n-\n-\n-\n-\n-\n-\n-\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n3000\nJW Wilson (AUS)\n1956-1956\n1\n-\n-\n-\n-\n-\n-\n-\n-\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\n\n\n3001 rows × 12 columns\n\n\n\nIdentify null data\n\ndata.apply(pd.isnull).sum()/data.shape[0]\n\nPlayer            0.0\nSpan              0.0\nMat               0.0\nInn               0.0\nNO                0.0\nRuns              0.0\nHS                0.0\nAvg               0.0\n100               0.0\n50                0.0\n0                 0.0\nPlayer Profile    0.0\ndtype: float64\n\n\n\ndata.describe(include='object')\n\n\n\n\n\n\n\n\nPlayer\nSpan\nInn\nNO\nRuns\nHS\nAvg\n100\n50\n0\nPlayer Profile\n\n\n\n\ncount\n3001\n3001\n3001\n3001\n3001\n3001\n3001\n3001\n3001\n3001\n3001\n\n\nunique\n2995\n1140\n198\n49\n1188\n465\n1598\n40\n57\n35\n3001\n\n\ntop\nImran Khan (PAK)\n2019-2019\n2\n0\n0\n5\n-\n0\n0\n0\nhttp://stats.espncricinfo.com/ci/content/playe...\n\n\nfreq\n2\n45\n374\n953\n52\n40\n88\n2203\n1619\n854\n1\n\n\n\n\n\n\n\n\ncricket=data[[\"Player\",\"Span\",\"Mat\",\"Inn\",\"Runs\",\"Avg\",\"100\",\"50\"]].copy()\ncricket.columns=[\"player\",\"span\",\"mat\",\"inn\",\"runs\",\"avg\",\"100\",\"50\"]\ncricket.head()\n\n\n\n\n\n\n\n\nplayer\nspan\nmat\ninn\nruns\navg\n100\n50\n\n\n\n\n0\nSR Tendulkar (INDIA)\n1989-2013\n200\n329\n15921\n53.78\n51\n68\n\n\n1\nRT Ponting (AUS)\n1995-2012\n168\n287\n13378\n51.85\n41\n62\n\n\n2\nJH Kallis (ICC/SA)\n1995-2013\n166\n280\n13289\n55.37\n45\n58\n\n\n3\nR Dravid (ICC/INDIA)\n1996-2012\n164\n286\n13288\n52.31\n36\n63\n\n\n4\nAN Cook (ENG)\n2006-2018\n161\n291\n12472\n45.35\n33\n57\n\n\n\n\n\n\n\nconverting to numerical int data\n\n# Convert the 'runs' column to a numeric data type (int)\ncricket['runs'] = cricket['runs'].str.replace(',', '', regex=True).str.extract('(\\d+)').astype(float)\n\n# Filter the data to select players with more than 5000 runs\nbatsman = cricket[cricket['runs'] &gt; 2000]\n\n# Display the filtered data in column form\nbatsman.head()\n\n\n\n\n\n\n\n\nplayer\nspan\nmat\ninn\nruns\navg\n100\n50\n\n\n\n\n0\nSR Tendulkar (INDIA)\n1989-2013\n200\n329\n15921.0\n53.78\n51\n68\n\n\n1\nRT Ponting (AUS)\n1995-2012\n168\n287\n13378.0\n51.85\n41\n62\n\n\n2\nJH Kallis (ICC/SA)\n1995-2013\n166\n280\n13289.0\n55.37\n45\n58\n\n\n3\nR Dravid (ICC/INDIA)\n1996-2012\n164\n286\n13288.0\n52.31\n36\n63\n\n\n4\nAN Cook (ENG)\n2006-2018\n161\n291\n12472.0\n45.35\n33\n57\n\n\n\n\n\n\n\n\n# Use .loc to update the 'Player' column\nbatsman.loc[:, 'player'] = batsman['player'].str.replace(r'\\s*\\(.*\\)', '', regex=True)\n\n# Display the DataFrame with the country names removed\n\nbatsman.tail()\n\n\n\n\n\n\n\n\nplayer\nspan\nmat\ninn\nruns\navg\n100\n50\n\n\n\n\n308\nCJL Rogers\n2008-2015\n25\n48\n2015.0\n42.87\n5\n14\n\n\n309\nKOA Powell\n2011-2018\n40\n76\n2011.0\n26.81\n3\n6\n\n\n310\nAC Hudson\n1992-1998\n35\n63\n2007.0\n33.45\n4\n13\n\n\n311\nKL Rahul\n2014-2019\n36\n60\n2006.0\n34.58\n5\n11\n\n\n312\nDN Sardesai\n1961-1972\n30\n55\n2001.0\n39.23\n5\n9\n\n\n\n\n\n\n\n\n# List of names to delete\n\n\n# Filter out rows with the specified names\n\n\nbatsman.head()\n\n\n\n\n\n\n\n\nplayer\nspan\nmat\ninn\nruns\navg\n100\n50\n\n\n\n\n0\nSR Tendulkar\n1989-2013\n200\n329\n15921.0\n53.78\n51\n68\n\n\n1\nRT Ponting\n1995-2012\n168\n287\n13378.0\n51.85\n41\n62\n\n\n2\nJH Kallis\n1995-2013\n166\n280\n13289.0\n55.37\n45\n58\n\n\n3\nR Dravid\n1996-2012\n164\n286\n13288.0\n52.31\n36\n63\n\n\n4\nAN Cook\n2006-2018\n161\n291\n12472.0\n45.35\n33\n57\n\n\n\n\n\n\n\n\n#to save the players name for future \nplayer = batsman['player'].tolist()\n\n\n# Split the 'span' values into start and end years\nbatsman[['Start_Year', 'End_Year']] = batsman['span'].str.split('-', expand=True).astype(int)\n\n# Calculate the duration in years\nbatsman['Span_yrs'] = batsman['End_Year'] - batsman['Start_Year'] + 1  # Adding 1 to include both start and end years\n\n# Display the DataFrame with the duration calculated\nbatsman\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_42556\\1235120657.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_42556\\1235120657.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_42556\\1235120657.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nplayer\nspan\nmat\ninn\nruns\navg\n100\n50\nStart_Year\nEnd_Year\nSpan_yrs\n\n\n\n\n0\nSR Tendulkar\n1989-2013\n200\n329\n15921.0\n53.78\n51\n68\n1989\n2013\n25\n\n\n1\nRT Ponting\n1995-2012\n168\n287\n13378.0\n51.85\n41\n62\n1995\n2012\n18\n\n\n2\nJH Kallis\n1995-2013\n166\n280\n13289.0\n55.37\n45\n58\n1995\n2013\n19\n\n\n3\nR Dravid\n1996-2012\n164\n286\n13288.0\n52.31\n36\n63\n1996\n2012\n17\n\n\n4\nAN Cook\n2006-2018\n161\n291\n12472.0\n45.35\n33\n57\n2006\n2018\n13\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n308\nCJL Rogers\n2008-2015\n25\n48\n2015.0\n42.87\n5\n14\n2008\n2015\n8\n\n\n309\nKOA Powell\n2011-2018\n40\n76\n2011.0\n26.81\n3\n6\n2011\n2018\n8\n\n\n310\nAC Hudson\n1992-1998\n35\n63\n2007.0\n33.45\n4\n13\n1992\n1998\n7\n\n\n311\nKL Rahul\n2014-2019\n36\n60\n2006.0\n34.58\n5\n11\n2014\n2019\n6\n\n\n312\nDN Sardesai\n1961-1972\n30\n55\n2001.0\n39.23\n5\n9\n1961\n1972\n12\n\n\n\n\n313 rows × 11 columns\n\n\n\n\n#sns.pairplot(batsman)\n#plt.show()\n\n\nbatsman = batsman.drop(['player'], axis = 1)\nbatsman.head()\n\n\n\n\n\n\n\n\nspan\nmat\ninn\nruns\navg\n100\n50\nStart_Year\nEnd_Year\nSpan_yrs\n\n\n\n\n0\n1989-2013\n200\n329\n15921.0\n53.78\n51\n68\n1989\n2013\n25\n\n\n1\n1995-2012\n168\n287\n13378.0\n51.85\n41\n62\n1995\n2012\n18\n\n\n2\n1995-2013\n166\n280\n13289.0\n55.37\n45\n58\n1995\n2013\n19\n\n\n3\n1996-2012\n164\n286\n13288.0\n52.31\n36\n63\n1996\n2012\n17\n\n\n4\n2006-2018\n161\n291\n12472.0\n45.35\n33\n57\n2006\n2018\n13\n\n\n\n\n\n\n\n\ncolumns=[\"runs\",\"avg\",\"100\"]\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\n# Initialize PCA with the number of components you want to retain\nn_components = 2  # Adjust as needed\npca = PCA(n_components=n_components)\n\n# Initialize the MinMaxScaler\nscaler = MinMaxScaler()\n\n# Apply Min-Max scaling to your data\nscaled_data = scaler.fit_transform(batsman[columns])\n\n# Fit and transform PCA on the scaled data\nbatter = pca.fit_transform(scaled_data)\n\nbatter\n\narray([[ 1.13670075e+00, -1.82729263e-01],\n       [ 8.75321769e-01, -1.30877512e-01],\n       [ 9.35142180e-01, -8.70045354e-02],\n       [ 8.08831970e-01, -1.26530872e-01],\n       [ 7.03738026e-01, -1.78727207e-01],\n       [ 8.09284250e-01, -4.18940850e-02],\n       [ 7.19007265e-01, -7.98234782e-02],\n       [ 6.58044190e-01, -9.63373931e-02],\n       [ 7.00634681e-01, -1.08532856e-01],\n       [ 5.82240976e-01, -8.54887301e-02],\n       [ 6.35566261e-01, -6.88147642e-02],\n       [ 6.21204290e-01, -4.15039454e-02],\n       [ 6.23562177e-01, -3.06646842e-02],\n       [ 4.86012399e-01, -6.80567045e-02],\n       [ 4.78482288e-01, -5.07045543e-02],\n       [ 3.49711940e-01, -1.05807708e-01],\n       [ 4.22201239e-01,  7.09635186e-03],\n       [ 4.36410575e-01, -2.37583474e-02],\n       [ 3.18311354e-01, -6.72827100e-02],\n       [ 3.98922157e-01, -1.22760487e-02],\n       [ 4.63460260e-01, -2.12393588e-02],\n       [ 4.94200326e-01, -1.54884051e-03],\n       [ 3.97784968e-01, -2.03217967e-02],\n       [ 4.11595125e-01, -8.49936300e-03],\n       [ 2.52759601e-01, -1.28646614e-01],\n       [ 2.97193751e-01, -6.80360904e-02],\n       [ 3.69858602e-01, -3.00213446e-02],\n       [ 3.55431580e-01, -2.38282724e-02],\n       [ 4.40235649e-01,  9.09731739e-02],\n       [ 3.03440651e-01, -8.68479045e-02],\n       [ 2.21940365e-01, -1.25007978e-01],\n       [ 3.17214012e-01, -6.64031613e-02],\n       [ 3.38137033e-01, -3.66661242e-02],\n       [ 3.17245763e-01, -4.82663512e-02],\n       [ 2.78200817e-01, -4.11042371e-02],\n       [ 3.69062571e-01,  4.56103502e-02],\n       [ 2.71924399e-01, -5.34496002e-02],\n       [ 2.83404989e-01, -1.85407673e-02],\n       [ 2.52763803e-01, -6.60081374e-02],\n       [ 2.92894703e-01, -4.70833527e-02],\n       [ 2.56694949e-01,  3.89172827e-03],\n       [ 2.92374334e-01, -2.52852532e-02],\n       [ 3.52777653e-01,  1.20050852e-01],\n       [ 2.00510975e-01, -6.07260431e-02],\n       [ 2.13121930e-01, -6.00755937e-02],\n       [ 4.01069794e-01,  8.71496429e-02],\n       [ 1.13944658e-01, -8.66499903e-02],\n       [ 4.16074387e-01,  1.73262952e-01],\n       [ 2.63970509e-01, -7.21896083e-03],\n       [ 3.54062899e-01,  7.58720777e-02],\n       [ 3.18936598e-01,  1.44001618e-02],\n       [ 2.63402775e-01, -6.48082748e-02],\n       [ 5.85703141e-01,  5.84233184e-01],\n       [ 1.67814319e-01, -7.68131906e-02],\n       [ 2.93985241e-01,  1.07322069e-01],\n       [ 2.08590232e-01, -4.90177477e-02],\n       [ 3.06051063e-01,  1.34954956e-01],\n       [ 1.99197620e-01, -1.82987624e-02],\n       [ 1.11038292e-01, -7.74426455e-02],\n       [ 2.70302899e-01,  7.03983125e-02],\n       [ 2.24753302e-01, -2.18777118e-02],\n       [ 2.37937445e-01,  7.43951700e-02],\n       [ 1.71516575e-01,  2.84534707e-02],\n       [ 2.50739437e-01,  6.50805930e-03],\n       [ 1.34761770e-01, -3.10642800e-02],\n       [ 2.47436703e-01,  4.46537845e-02],\n       [ 1.30354330e-01, -2.85513395e-02],\n       [ 1.58594113e-01,  3.71522229e-03],\n       [ 1.48815318e-01, -1.33142381e-02],\n       [ 1.24019178e-01, -4.12727110e-02],\n       [ 1.24663098e-01, -3.13294468e-04],\n       [ 1.85629487e-01,  7.05519019e-02],\n       [ 1.34196351e-01,  1.08424191e-03],\n       [ 9.67268701e-02, -7.03084841e-02],\n       [ 8.11669492e-02, -7.87750074e-02],\n       [ 1.92856717e-01,  6.70414073e-02],\n       [ 1.61525688e-01, -1.97660556e-02],\n       [ 5.51982786e-02,  8.72297332e-04],\n       [ 1.64561828e-01,  5.12280579e-02],\n       [-5.63216323e-02, -1.43608765e-01],\n       [ 1.16108164e-01, -4.06753844e-02],\n       [ 1.22993217e-01, -1.90388395e-02],\n       [ 1.25308938e-01,  6.51478158e-02],\n       [ 1.49850038e-01,  3.08138115e-02],\n       [ 1.66280553e-01,  1.56465921e-01],\n       [ 1.30945332e-01,  6.37002756e-02],\n       [ 9.56012752e-02, -1.52488738e-04],\n       [ 5.22296764e-02, -5.12432400e-02],\n       [ 9.55024657e-02,  5.34358444e-03],\n       [-2.85469478e-02, -1.24980298e-01],\n       [ 9.51430248e-02,  5.40921220e-02],\n       [ 5.43044368e-02,  4.66135110e-02],\n       [ 5.49316125e-02, -9.22064740e-02],\n       [ 6.40162889e-02,  1.71357227e-02],\n       [-6.91822648e-02, -7.27998661e-02],\n       [ 6.49404659e-02,  3.31196594e-02],\n       [ 4.16848801e-02,  3.36121818e-02],\n       [-4.60483317e-02, -3.81102827e-02],\n       [ 3.74427508e-02,  5.97595090e-02],\n       [ 7.70217291e-02,  1.14963654e-01],\n       [-7.28308233e-03,  2.59562426e-02],\n       [ 4.99800572e-03, -4.08073534e-02],\n       [ 3.74489464e-02, -4.73916263e-02],\n       [ 1.50730948e-01,  2.25227924e-01],\n       [ 6.29566251e-02,  4.41465395e-02],\n       [ 1.92162840e-02,  2.77618437e-02],\n       [ 5.90050071e-02,  7.18336680e-02],\n       [-9.37010533e-02, -1.15292862e-01],\n       [ 1.66533479e-02, -1.27832089e-02],\n       [ 1.04882160e-02,  8.23306757e-02],\n       [ 1.25017619e-01,  2.04606664e-01],\n       [-3.51686429e-02, -3.53392336e-02],\n       [-3.33203130e-02, -2.89499349e-02],\n       [-2.78771796e-02, -4.82999549e-02],\n       [ 5.09752212e-02,  7.15935693e-02],\n       [ 3.05707908e-02,  5.00116844e-02],\n       [-1.03161259e-01, -8.16155787e-02],\n       [ 9.50690872e-03,  2.89779826e-02],\n       [-1.37742915e-01, -1.39587368e-01],\n       [-3.09663426e-02,  2.81607412e-02],\n       [-3.17882804e-02, -9.10811903e-03],\n       [-6.30525410e-02, -5.64478295e-02],\n       [-1.64744822e-02,  1.09112487e-02],\n       [-1.86531070e-02,  4.47700974e-02],\n       [-9.05495012e-02, -4.99532519e-02],\n       [-3.17212656e-03,  6.17683961e-02],\n       [-2.92151942e-02,  2.86319397e-02],\n       [-3.29987356e-02, -1.14088300e-02],\n       [ 9.32502214e-04,  5.08163325e-02],\n       [-6.41176000e-02,  9.70949714e-03],\n       [ 4.64981022e-03,  7.20639348e-02],\n       [-1.00793197e-01, -4.80027367e-02],\n       [-1.33594606e-02, -3.80604174e-03],\n       [-6.44810409e-02,  1.97124662e-03],\n       [-1.01582093e-01, -6.66054740e-02],\n       [ 4.77527864e-03,  6.03066419e-02],\n       [-1.04353401e-01,  7.26931013e-03],\n       [-1.54904517e-02,  1.19777513e-01],\n       [-2.13155136e-02,  3.31641861e-02],\n       [-1.33940851e-01, -7.52033347e-02],\n       [-3.70762153e-02,  6.18159521e-02],\n       [-4.30558222e-02, -2.68204701e-02],\n       [-1.00785162e-01, -8.91099262e-03],\n       [-4.93782781e-02,  3.25936224e-02],\n       [ 8.50327792e-02,  2.04230466e-01],\n       [-8.37052893e-02, -2.83218323e-02],\n       [-1.73338229e-01, -6.94094621e-02],\n       [-1.39483037e-01, -3.51159593e-02],\n       [-1.10237579e-01, -1.99171854e-02],\n       [-2.79644061e-02,  9.37538280e-03],\n       [-2.92358982e-02,  4.19998092e-02],\n       [-5.76987734e-02, -1.80434866e-03],\n       [-5.23836652e-02,  1.51228734e-02],\n       [-1.89807213e-01, -1.27165736e-01],\n       [-1.24337466e-02,  9.64824283e-02],\n       [-1.59959287e-02,  5.00731529e-02],\n       [-7.71151065e-02,  2.06695374e-02],\n       [-1.43215467e-02,  9.43590174e-02],\n       [-7.53248253e-02,  2.88183343e-02],\n       [-4.42268526e-02,  1.44722844e-02],\n       [-3.03558681e-02,  5.82119166e-02],\n       [-4.82905409e-03,  9.94931287e-02],\n       [-6.46366057e-02,  1.08772634e-01],\n       [-9.02768209e-02,  1.92891162e-03],\n       [-1.07348836e-01, -4.56245038e-02],\n       [-4.98687428e-02,  1.24762562e-01],\n       [-1.48912865e-01, -8.65893814e-02],\n       [-1.26517686e-01, -5.64549279e-02],\n       [-1.36268939e-01, -4.49920305e-02],\n       [-1.01980107e-01,  2.07186248e-02],\n       [-1.45892677e-01, -6.49023852e-02],\n       [-1.06175106e-01, -5.69822619e-02],\n       [-1.59342903e-01, -5.69394183e-02],\n       [-1.53453225e-01, -3.95791577e-02],\n       [-4.88700215e-02,  1.04917933e-02],\n       [-1.45727854e-01, -1.07854340e-02],\n       [-1.06352192e-01,  3.26190214e-02],\n       [-1.70962087e-01, -4.45642294e-02],\n       [-1.42249461e-01,  4.51630471e-03],\n       [-7.55115604e-02,  9.02981428e-02],\n       [-1.12477158e-01,  2.61421179e-02],\n       [-7.59413286e-02,  6.04689287e-02],\n       [-8.69020379e-02,  3.42445608e-02],\n       [-1.02271036e-01,  2.73806094e-02],\n       [-2.68864590e-01, -2.00029327e-01],\n       [-2.86520521e-01, -2.14198910e-01],\n       [-2.25452276e-01, -1.04815797e-01],\n       [-1.32292554e-01,  2.11530049e-02],\n       [-1.42249437e-01,  3.00948794e-02],\n       [-1.02626094e-01,  7.28130956e-02],\n       [-2.50641097e-01, -1.35309927e-01],\n       [-5.29295038e-02,  1.13407733e-01],\n       [-1.58590267e-01, -9.03978657e-03],\n       [-2.03608949e-01, -6.06827622e-02],\n       [-1.26924024e-01,  5.48780624e-02],\n       [-1.43919725e-01,  4.55935992e-02],\n       [-1.02489592e-01,  9.30043864e-02],\n       [-1.28970950e-01,  2.14295364e-02],\n       [-4.39949198e-02,  1.95130005e-01],\n       [-1.33024424e-01,  1.05987490e-02],\n       [-1.19805928e-01,  5.26291385e-02],\n       [-1.07319127e-01,  9.11447684e-02],\n       [-2.16068579e-01, -1.10363602e-01],\n       [-2.40983666e-01, -1.46198996e-01],\n       [-1.09911988e-01,  5.36668034e-02],\n       [-1.92343195e-01, -3.90396382e-02],\n       [-2.41663811e-01, -1.06260465e-01],\n       [-1.44061806e-01,  3.14868431e-02],\n       [-2.38509215e-01, -9.59233679e-02],\n       [-1.90723968e-01, -2.94577108e-02],\n       [-2.22351297e-01, -4.48781044e-02],\n       [-1.71609027e-01, -4.94255754e-03],\n       [-1.81142012e-01,  3.99719854e-03],\n       [-1.38848287e-01,  2.02334417e-02],\n       [-1.61748373e-01, -7.32973325e-03],\n       [-1.39057322e-01,  5.90798438e-02],\n       [-1.97419319e-01, -7.23017753e-02],\n       [-1.22366374e-01,  1.08861418e-01],\n       [-1.61641041e-01, -4.21662694e-03],\n       [-1.50950310e-01,  9.90770766e-02],\n       [-8.29475683e-02,  1.16969853e-01],\n       [-2.44079212e-01, -9.46656428e-02],\n       [-1.14664628e-01,  6.28913691e-02],\n       [-1.13890217e-01,  1.38772301e-01],\n       [-2.13791166e-01, -7.62805955e-02],\n       [-2.00066472e-01, -3.62921717e-02],\n       [-2.05633018e-01, -1.24261225e-01],\n       [-1.58233136e-01,  5.05068454e-02],\n       [-1.44933514e-01,  1.98279429e-02],\n       [-1.17860810e-01,  9.87168979e-02],\n       [-1.93899118e-01, -4.49660165e-02],\n       [-1.65019623e-01,  3.86155096e-02],\n       [-1.96197514e-01,  2.34419613e-02],\n       [-1.25169178e-01,  4.98977030e-02],\n       [-2.01190115e-01,  1.09500683e-02],\n       [-1.84487903e-01, -4.76000375e-02],\n       [-1.68304443e-01,  3.69418970e-03],\n       [-2.36230756e-01, -4.60697666e-02],\n       [-1.90865514e-01, -2.28898820e-02],\n       [-2.31131020e-01, -6.30828384e-02],\n       [-1.81762411e-01,  8.48127980e-03],\n       [-2.35609682e-01, -7.44408202e-02],\n       [-2.19295103e-01, -6.32691992e-02],\n       [-1.43144301e-01,  8.56440727e-02],\n       [-2.02597702e-01, -1.08410609e-02],\n       [-2.10164191e-01,  6.93161286e-03],\n       [-1.84098558e-01, -2.59777347e-02],\n       [-1.27391701e-01,  1.39197844e-01],\n       [-1.34746574e-01,  1.19182741e-01],\n       [-2.40977524e-01, -7.01386845e-03],\n       [-3.04343935e-01, -1.88269267e-01],\n       [-1.70483822e-01,  5.76983105e-02],\n       [-2.25509960e-01,  9.59529205e-03],\n       [-1.70884643e-01, -1.32493649e-02],\n       [-1.56938123e-01,  6.33259168e-02],\n       [-2.45819849e-01, -8.43067466e-02],\n       [-1.50875008e-01,  4.76936015e-02],\n       [-2.00826441e-01, -2.25754423e-02],\n       [-1.60152570e-01,  5.91229952e-02],\n       [-1.39644679e-01,  8.22300480e-02],\n       [-2.84686151e-01, -1.55879310e-01],\n       [-1.32845485e-01,  6.67964409e-02],\n       [-2.23402731e-01, -4.51739753e-02],\n       [-2.30523652e-01, -2.87423947e-02],\n       [-2.30839904e-01, -6.31492308e-02],\n       [-2.63390363e-01, -7.87829661e-02],\n       [-2.53824405e-01, -4.67758206e-02],\n       [-1.51330876e-01,  7.03787668e-02],\n       [-2.38368456e-01, -3.49304754e-02],\n       [-1.31774552e-01,  1.29486949e-01],\n       [-2.70010492e-01, -5.12693201e-02],\n       [-2.00049112e-01,  4.24206292e-02],\n       [-2.51789420e-01, -1.05608031e-01],\n       [-2.37992284e-01, -2.89385931e-02],\n       [-1.90299859e-01,  2.71433765e-03],\n       [-7.75759880e-02,  2.93725403e-01],\n       [-2.72648299e-01, -4.90765877e-02],\n       [-3.03942638e-01, -1.73826635e-01],\n       [-2.21223814e-01, -4.31169568e-02],\n       [-2.37104171e-01, -5.00561938e-02],\n       [-2.68872316e-01, -1.04632605e-01],\n       [-2.42667274e-01, -2.87751158e-02],\n       [-1.30937058e-01,  1.50826785e-01],\n       [-4.31451338e-02,  2.96363186e-01],\n       [-2.66167671e-01, -8.96383641e-02],\n       [-1.90864464e-01,  9.18972158e-02],\n       [-1.50405965e-01,  1.39655359e-01],\n       [-1.56085829e-01,  8.88819993e-02],\n       [-2.40935751e-01, -4.65809019e-02],\n       [-1.32275563e-01,  1.59224741e-01],\n       [-2.25448586e-01, -3.72733315e-02],\n       [-2.73814192e-01, -3.12451566e-02],\n       [-2.21513790e-01, -2.39821732e-02],\n       [-2.03866113e-01, -8.08447755e-03],\n       [-2.59223011e-01, -5.88947121e-02],\n       [-2.09465593e-01,  1.36171240e-02],\n       [-2.86123021e-01, -2.55919894e-02],\n       [-2.56753409e-01, -4.82583960e-02],\n       [-3.09616140e-01, -1.26234724e-01],\n       [-2.68088023e-01, -4.18939597e-02],\n       [-1.66007669e-01,  1.44733391e-01],\n       [-2.01302909e-01,  4.27978675e-02],\n       [-2.52625109e-01, -3.23060403e-02],\n       [-1.53601252e-01,  1.10774059e-01],\n       [-2.69880419e-01, -8.14601869e-03],\n       [-2.16190473e-01,  3.90762892e-02],\n       [-2.62325277e-01, -2.03452153e-02],\n       [-2.54593920e-01, -3.18602911e-02],\n       [-1.83252540e-01,  1.02987783e-01],\n       [-2.69443502e-01, -7.29951410e-02],\n       [-2.31882686e-01,  6.29274598e-05],\n       [-2.14927746e-01,  1.30839656e-02],\n       [-1.97660937e-01,  6.38278913e-02]])\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Explained variance ratio:\", explained_variance)\n\nExplained variance ratio: [0.88180887 0.10154978]\n\n\n\nplt.scatter(batter[:, 0], batter[:, 1])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Results')\nplt.show()\n\n\n\n\n\ncomponent_loadings = pca.components_\nprint(\"Principal component loadings:\", component_loadings)\n\nPrincipal component loadings: [[ 0.69313653  0.31121587  0.65015878]\n [-0.43688304  0.89881675  0.03551987]]\n\n\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n\n# Your data\nX = batter  # Your data points\n\nbest_eps = None\nbest_min_samples = None\nbest_score = -1\n\nfor eps in np.arange(0.1, 1.0, 0.1):  # Adjust the range as needed\n    for min_samples in range(2, 11):  # Adjust the range as needed\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(X)\n        if len(set(labels)) &gt; 1:  # Ensure more than one cluster is formed\n            score = silhouette_score(X, labels)\n            if score &gt; best_score:\n                best_score = score\n                best_eps = eps\n                best_min_samples = min_samples\n\nprint(f\"Best eps: {best_eps}, Best min_samples: {best_min_samples}, Best Silhouette Score: {best_score}\")\n\nBest eps: 0.2, Best min_samples: 2, Best Silhouette Score: 0.6825550599476355\n\n\n\nfrom sklearn.cluster import DBSCAN\n\n# Initialize DBSCAN\ndbscan = DBSCAN(eps=0.2, min_samples=2)\n\n# Fit DBSCAN on the PCA-transformed data\ncluster_labels = dbscan.fit_predict(batter)\n\n\nplt.scatter(batter[:, 0], batter[:, 1], c=cluster_labels, cmap='viridis')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('DBSCAN Clustering Results')\nplt.show()\n\n\n\n\n\n# Assuming 'player' contains the player names and 'cluster_labels' contains the cluster assignments\n# Create a new DataFrame to combine the results\nnew_batsman = pd.DataFrame({'x': batter[:, 0], 'y': batter[:, 1], 'cluster': cluster_labels, 'player': player})\n\n# Display the resulting DataFrame\nnew_batsman\n\n\n\n\n\n\n\n\nx\ny\ncluster\nplayer\n\n\n\n\n0\n1.136701\n-0.182729\n-1\nSR Tendulkar\n\n\n1\n0.875322\n-0.130878\n0\nRT Ponting\n\n\n2\n0.935142\n-0.087005\n0\nJH Kallis\n\n\n3\n0.808832\n-0.126531\n0\nR Dravid\n\n\n4\n0.703738\n-0.178727\n0\nAN Cook\n\n\n...\n...\n...\n...\n...\n\n\n308\n-0.183253\n0.102988\n0\nCJL Rogers\n\n\n309\n-0.269444\n-0.072995\n0\nKOA Powell\n\n\n310\n-0.231883\n0.000063\n0\nAC Hudson\n\n\n311\n-0.214928\n0.013084\n0\nKL Rahul\n\n\n312\n-0.197661\n0.063828\n0\nDN Sardesai\n\n\n\n\n313 rows × 4 columns\n\n\n\n\n# Create a scatter plot with cluster labels\nplt.figure(figsize=(10, 6))\nax = sns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster\", data=new_batsman, palette=\"viridis\", s=100)\n\n# Add labels for individual data points\nfor x, y, player, cluster in zip(new_batsman['x'], new_batsman['y'], new_batsman['player'], new_batsman['cluster']):\n    plt.text(x, y, player, fontsize=10, alpha=0.8)\n\n# Set the plot limits and labels\nax.set(ylim=(-3, 3))\nplt.xlabel(\"Principal Component 1\", fontsize=15)\nplt.ylabel(\"Principal Component 2\", fontsize=15)\n\n# Show the legend\nplt.legend(title='Cluster', loc='upper right', labels=[f'Cluster {label}' for label in new_batsman['cluster'].unique()])\n\n# Display the plot\nplt.show()\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Set the style\nsns.set(style=\"white\")\n\n# Normalize the \"average\" values\nscaler = MinMaxScaler()\nbatsman['avg_normalized'] = scaler.fit_transform(batsman[['avg']])\n\n# Create a scatter plot with cluster labels and manually set marker size based on \"average\" values\nplt.figure(figsize=(10, 6))\nax = sns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster\", data=new_batsman, palette=\"viridis\", sizes=(50, 500), size=batsman['avg_normalized'])\n\n# Add labels for individual data points\nfor x, y, player, cluster in zip(new_batsman['x'], new_batsman['y'], new_batsman['player'], new_batsman['cluster']):\n    plt.text(x, y, player, fontsize=10, alpha=0.8)\n\n# Set the plot limits and labels\nax.set(ylim=(-3, 3))\nplt.xlabel(\"Principal Component 1\", fontsize=15)\nplt.ylabel(\"Principal Component 2\", fontsize=15)\n\n# Show the legend\nplt.legend(title='Cluster', loc='best', labels=[f'Cluster {label}' for label in new_batsman['cluster'].unique()])\n\n# Display the plot\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Set the style\nsns.set(style=\"white\")\n\n# Normalize the \"average\" values\nscaler = MinMaxScaler()\nbatsman['100_normalized'] = scaler.fit_transform(batsman[['100']])\n\n# Create a scatter plot with cluster labels and manually set marker size based on \"average\" values\nplt.figure(figsize=(10, 6))\nax = sns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster\", data=new_batsman, palette=\"viridis\", sizes=(50, 500), size=batsman['100_normalized'])\n\n# Add labels for individual data points\nfor x, y, player, cluster in zip(new_batsman['x'], new_batsman['y'], new_batsman['player'], new_batsman['cluster']):\n    plt.text (x, y, player, fontsize=10, alpha=0.8)\n\n# Set the plot limits and labels\nax.set(ylim=(-3, 3))\nplt.xlabel(\"Principal Component 1\", fontsize=15)\nplt.ylabel(\"Principal Component 2\", fontsize=15)\n\n# Show the legend\nplt.legend(title='Cluster', loc='upper right', labels=[f'Cluster {label}' for label in new_batsman['cluster'].unique()])\n\n# Display the plot\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndata=pd.read_csv('air_quality_data.csv')\ndata.head()\n\n\n\n\n\n\n\n\nState\nCounty\nYear\nDays with AQI\nGood Days\nModerate Days\nUnhealthy for Sensitive Groups Days\nUnhealthy Days\nVery Unhealthy Days\nHazardous Days\nMax AQI\n90th Percentile AQI\nMedian AQI\nDays CO\nDays NO2\nDays Ozone\nDays PM2.5\nDays PM10\n\n\n\n\n0\nAlabama\nBaldwin\n2022\n141\n119\n22\n0\n0\n0\n0\n96\n52\n40\n0\n0\n114\n27\n0\n\n\n1\nAlabama\nClay\n2022\n58\n50\n8\n0\n0\n0\n0\n64\n52\n27\n0\n0\n0\n58\n0\n\n\n2\nAlabama\nDeKalb\n2022\n242\n225\n17\n0\n0\n0\n0\n97\n48\n37\n0\n0\n224\n18\n0\n\n\n3\nAlabama\nElmore\n2022\n117\n110\n7\n0\n0\n0\n0\n67\n47\n37\n0\n0\n117\n0\n0\n\n\n4\nAlabama\nEtowah\n2022\n179\n140\n39\n0\n0\n0\n0\n93\n58\n42\n0\n0\n76\n103\n0\n\n\n\n\n\n\n\n\ndata.apply(pd.isnull).sum()/data.shape[0]\n\nState                                  0.0\nCounty                                 0.0\nYear                                   0.0\nDays with AQI                          0.0\nGood Days                              0.0\nModerate Days                          0.0\nUnhealthy for Sensitive Groups Days    0.0\nUnhealthy Days                         0.0\nVery Unhealthy Days                    0.0\nHazardous Days                         0.0\nMax AQI                                0.0\n90th Percentile AQI                    0.0\nMedian AQI                             0.0\nDays CO                                0.0\nDays NO2                               0.0\nDays Ozone                             0.0\nDays PM2.5                             0.0\nDays PM10                              0.0\ndtype: float64\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nYear\nDays with AQI\nGood Days\nModerate Days\nUnhealthy for Sensitive Groups Days\nUnhealthy Days\nVery Unhealthy Days\nHazardous Days\nMax AQI\n90th Percentile AQI\nMedian AQI\nDays CO\nDays NO2\nDays Ozone\nDays PM2.5\nDays PM10\n\n\n\n\ncount\n966.0\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n966.000000\n\n\nmean\n2022.0\n208.917184\n173.689441\n33.388199\n1.566253\n0.214286\n0.026915\n0.032091\n112.774327\n55.115942\n36.821946\n0.666667\n4.330228\n127.083851\n68.909938\n7.926501\n\n\nstd\n0.0\n58.371700\n52.564146\n29.654791\n4.591190\n1.401200\n0.267981\n0.419727\n254.401084\n14.632954\n9.672426\n5.107616\n19.372906\n82.596442\n69.893621\n30.194461\n\n\nmin\n2022.0\n4.000000\n4.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n5.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2022.0\n181.000000\n147.250000\n12.000000\n0.000000\n0.000000\n0.000000\n0.000000\n77.000000\n48.000000\n35.000000\n0.000000\n0.000000\n76.000000\n0.000000\n0.000000\n\n\n50%\n2022.0\n212.000000\n174.000000\n24.000000\n0.000000\n0.000000\n0.000000\n0.000000\n95.000000\n54.000000\n39.000000\n0.000000\n0.000000\n131.500000\n58.000000\n0.000000\n\n\n75%\n2022.0\n261.750000\n213.000000\n46.750000\n1.000000\n0.000000\n0.000000\n0.000000\n115.000000\n61.000000\n42.000000\n0.000000\n0.000000\n188.000000\n107.000000\n0.000000\n\n\nmax\n2022.0\n306.000000\n293.000000\n197.000000\n57.000000\n20.000000\n6.000000\n10.000000\n7577.000000\n151.000000\n77.000000\n77.000000\n209.000000\n299.000000\n303.000000\n299.000000\n\n\n\n\n\n\n\n\nair=data[[\"State\",\"County\",\"Max AQI\",\"90th Percentile AQI\",\"Days PM2.5\"]].copy()\nair.columns=[\"State\",\"County\",\"Max AQI\",\"90th Percentile AQI\",\"Days PM2.5\"]\nair.head()\n\n\n\n\n\n\n\n\nState\nCounty\nMax AQI\n90th Percentile AQI\nDays PM2.5\n\n\n\n\n0\nAlabama\nBaldwin\n96\n52\n27\n\n\n1\nAlabama\nClay\n64\n52\n58\n\n\n2\nAlabama\nDeKalb\n97\n48\n18\n\n\n3\nAlabama\nElmore\n67\n47\n0\n\n\n4\nAlabama\nEtowah\n93\n58\n103\n\n\n\n\n\n\n\n\n#to save these features for the future\nState = air['State'].tolist()\nCounty = air['County'].tolist()\n\n\n#sns.pairplot(air)\n#plt.show()\n\n\nair = pd.DataFrame(air)\n\n# Drop the \"State\" and \"County\" columns\nair = air.drop(columns=[\"State\", \"County\"])\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize the Min-Max scaler\nscaler = MinMaxScaler()\n\n# Fit and transform the entire \"air\" dataset\nair = scaler.fit_transform(air)\n\n# \"normalized_air\" now contains the scaled features in the [0, 1] range\nair\n\narray([[0.01162637, 0.32191781, 0.08910891],\n       [0.0073986 , 0.32191781, 0.19141914],\n       [0.01175849, 0.29452055, 0.05940594],\n       ...,\n       [0.01215484, 0.29452055, 0.00660066],\n       [0.00620954, 0.08219178, 0.        ],\n       [0.00660589, 0.26712329, 0.        ]])\n\n\n\nfrom sklearn.decomposition import PCA\n\n# Initialize PCA with the desired number of components (e.g., 2 for a 2D visualization)\nnum_components = 2\npca = PCA(n_components=num_components)\n\n# Fit and transform your normalized data with PCA\nnew_air = pca.fit_transform(air)\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\n# Assuming you have normalized your data and stored it in 'normalized_air'\n\n# Determine the number of nearest neighbors (k) for the k-distance plot\nk = 5  # You can adjust this value\n\n# Fit a Nearest Neighbors model to the normalized data\nnn_model = NearestNeighbors(n_neighbors=k)\nnn_model.fit(air)\n\n# Calculate distances to the k-th nearest neighbor for each data point\ndistances, _ = nn_model.kneighbors(air)\n\n# Sort the distances and create a k-distance plot\nsorted_distances = np.sort(distances[:, -1])  # Sort by the distance to the k-th neighbor\nplt.plot(np.arange(1, len(sorted_distances) + 1), sorted_distances)\nplt.xlabel(\"Data Point Index\")\nplt.ylabel(f\"Distance to {k}-th Nearest Neighbor\")\nplt.title(f\"{k}-Distance Plot\")\nplt.grid(True)\n\n# Display the plot\nplt.show()\n\n\n\n\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n\n# Your data\nX = new_air  # Your data points\n\nbest_eps = None\nbest_min_samples = None\nbest_score = -1\n\nfor eps in np.arange(0.1, 1.0, 0.1):  # Adjust the range as needed\n    for min_samples in range(2, 20):  # Adjust the range as needed\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(X)\n        if len(set(labels)) &gt; 1:  # Ensure more than one cluster is formed\n            score = silhouette_score(X, labels)\n            if score &gt; best_score:\n                best_score = score\n                best_eps = eps\n                best_min_samples = min_samples\n\nprint(f\"Best eps: {best_eps}, Best min_samples: {best_min_samples}, Best Silhouette Score: {best_score}\")\n\nBest eps: 0.2, Best min_samples: 10, Best Silhouette Score: 0.5989772983728686\n\n\n\nfrom sklearn.cluster import DBSCAN\n\n# Initialize the DBSCAN model with your chosen parameters\ndbscan = DBSCAN(eps=0.05, min_samples=5)\n\n# Fit the model to the PCA-transformed data\ndbscan.fit(new_air)\n\n# Access the cluster labels assigned to each data point\ncluster_labels = dbscan.labels_\n\n\n\n# Plot the clusters using the first two principal components\nplt.figure(figsize=(10, 6))\nplt.scatter(new_air[:, 0], new_air[:, 1], c=cluster_labels, cmap='viridis')\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"DBSCAN Clustering Results after PCA\")\nplt.colorbar()\nplt.show()\n\n\n\n\n\n# Assuming you have cluster labels and PCA-transformed data\n# Create a DataFrame that includes the cluster labels\ndata_with_clusters = pd.DataFrame({\n    'Cluster': cluster_labels,\n    'PCA Component 1': new_air[:, 0],\n    'PCA Component 2': new_air[:, 1]\n})\n\n# Create a box plot for PCA Component 1 by cluster\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Cluster', y='PCA Component 1', data=data_with_clusters)\nplt.xlabel('Cluster')\nplt.ylabel('PCA Component 1')\nplt.title('Box Plot of PCA Component 1 by Cluster')\nplt.show()\n\n# Create a box plot for PCA Component 2 by cluster\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Cluster', y='PCA Component 2', data=data_with_clusters)\nplt.xlabel('Cluster')\nplt.ylabel('PCA Component 2')\nplt.title('Box Plot of PCA Component 2 by Cluster')\nplt.show()\n\n\n\n\n\n\n\n\n# Create a new DataFrame to combine the results\nnew_air = pd.DataFrame({'x': new_air[:, 0], 'y': new_air[:, 1], 'Cluster': cluster_labels, 'State': State})\n\n# Display the resulting DataFrame\nnew_air\n\n\n\n\n\n\n\n\nx\ny\nCluster\nState\n\n\n\n\n0\n-0.138111\n-0.022733\n0\nAlabama\n\n\n1\n-0.035805\n-0.022062\n0\nAlabama\n\n\n2\n-0.167556\n-0.050336\n0\nAlabama\n\n\n3\n-0.226895\n-0.057990\n0\nAlabama\n\n\n4\n0.112319\n0.020583\n0\nAlabama\n\n\n...\n...\n...\n...\n...\n\n\n961\n-0.224044\n-0.009787\n0\nWyoming\n\n\n962\n-0.125231\n0.012099\n0\nWyoming\n\n\n963\n-0.220359\n-0.050802\n0\nWyoming\n\n\n964\n-0.224973\n-0.263103\n0\nWyoming\n\n\n965\n-0.226703\n-0.078571\n0\nWyoming\n\n\n\n\n966 rows × 4 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming you have cluster labels, PCA-transformed data, and State information\n# Create a DataFrame that includes cluster labels, PCA components, and State\ndata_with_clusters = pd.DataFrame({\n    'Cluster': cluster_labels,\n    'PCA Component 1': new_air['x'],  # Assuming 'x' represents PCA Component 1\n    'PCA Component 2': new_air['y'],  # Assuming 'y' represents PCA Component 2\n    'State': State  # Assuming 'State' is available in your data\n})\n\n# Get unique cluster labels\nunique_clusters = data_with_clusters['Cluster'].unique()\n\n# Iterate through clusters and create individual scatter plots\nfor cluster in unique_clusters:\n    plt.figure(figsize=(10, 6))\n    ax = sns.scatterplot(\n        x=\"PCA Component 1\",\n        y=\"PCA Component 2\",\n        data=data_with_clusters[data_with_clusters['Cluster'] == cluster],  # Filter data by cluster\n        palette=\"viridis\",\n        s=100,\n    )\n\n    # Add labels for individual data points\n    for x, y, state in zip(\n        data_with_clusters[data_with_clusters['Cluster'] == cluster]['PCA Component 1'],\n        data_with_clusters[data_with_clusters['Cluster'] == cluster]['PCA Component 2'],\n        data_with_clusters[data_with_clusters['Cluster'] == cluster]['State'],\n    ):\n        plt.text(x, y, state, fontsize=10, alpha=0.8)\n\n    # Set the plot limits and labels\n    ax.set(ylim=(-3, 3))\n    plt.xlabel(\"Principal Component 1\", fontsize=15)\n    plt.ylabel(\"Principal Component 2\", fontsize=15)\n\n    # Set the title for the individual cluster plot\n    plt.title(f'Scatter Plot for Cluster {cluster}', fontsize=15)\n\n    # Display the plot\n    plt.show()\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_25252\\1173522113.py:19: UserWarning:\n\nIgnoring `palette` because no `hue` variable has been assigned.\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_25252\\1173522113.py:19: UserWarning:\n\nIgnoring `palette` because no `hue` variable has been assigned.\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_25252\\1173522113.py:19: UserWarning:\n\nIgnoring `palette` because no `hue` variable has been assigned.\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_25252\\1173522113.py:19: UserWarning:\n\nIgnoring `palette` because no `hue` variable has been assigned.\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_25252\\1173522113.py:19: UserWarning:\n\nIgnoring `palette` because no `hue` variable has been assigned.\n\nC:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_25252\\1173522113.py:19: UserWarning:\n\nIgnoring `palette` because no `hue` variable has been assigned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Assuming you have cluster labels, PCA-transformed data, and State information\n# Create a DataFrame that includes cluster labels, PCA components, and State\ndata_with_clusters = pd.DataFrame({\n    'Cluster': cluster_labels,\n    'PCA Component 1': new_air['x'],  # Assuming 'x' represents PCA Component 1\n    'PCA Component 2': new_air['y'],  # Assuming 'y' represents PCA Component 2\n    'State': State  # Assuming 'State' is available in your data\n})\n\n# Get unique cluster labels\nunique_clusters = data_with_clusters['Cluster'].unique()\n\n# Iterate through clusters and create individual bar plots for the 'State' variable\nfor cluster in unique_clusters:\n    plt.figure(figsize=(10, 6))\n    \n    # Count the occurrences of each 'State' within the cluster\n    state_counts = data_with_clusters[data_with_clusters['Cluster'] == cluster]['State'].value_counts()\n    \n    # Create a bar plot for the 'State' variable within the cluster\n    state_counts.plot(kind='bar', color='teal')\n    \n    plt.xlabel(\"State\", fontsize=15)\n    plt.ylabel(\"Count\", fontsize=15)\n    plt.title(f'Bar Plot for States in Cluster {cluster}', fontsize=15)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n# Compute silhouette scores for each data point\nsilhouette_avg = silhouette_score(new_air[['x', 'y']], cluster_labels)\nsample_silhouette_values = silhouette_samples(new_air[['x', 'y']], cluster_labels)\n\n# Add silhouette scores to the DataFrame\nnew_air['Silhouette Score'] = sample_silhouette_values\n\nprint(f\"Silhouette Score: {silhouette_avg}\")\n\nSilhouette Score: 0.14829996909281098\n\n\n\nimport numpy as np\n\n# Create a bar plot to visualize the silhouette scores by cluster\nplt.figure(figsize=(10, 6))\nax = sns.barplot(x=cluster_labels, y=new_air['Silhouette Score'], palette=\"viridis\")\nplt.xlabel(\"Cluster\", fontsize=15)\nplt.ylabel(\"Silhouette Score\", fontsize=15)\nplt.title(\"Silhouette Scores by Cluster\", fontsize=15)\n\n# Draw a vertical line at the average silhouette score\nplt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", label=\"Average Silhouette Score\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n#Max AQI (Maximum Air Quality Index): Clusters could represent groups of states with similar maximum air quality values. For example, a cluster might contain states that frequently experience high maximum AQI values, indicating occasional poor air quality.\n\n#90th Percentile AQI: This feature reflects the 90th percentile of AQI values, which indicates the AQI level exceeded only 10% of the time. Clusters might group states with similar patterns of exceeding AQI levels.\n\n#Days PM2.5, Days Ozone, Days CO: These features represent the number of days when specific air pollutants (PM2.5, Ozone, CO) exceed certain thresholds. Clusters could represent states with similar distributions of days exceeding these thresholds."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression",
    "section": "",
    "text": "This is a post with executable code.\n\n#importing the needed libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#importing the dataset\ndata=pd.read_csv('insurance.csv',sep=',')\ndata.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\ninsurance=data[['age','bmi','children','charges']].copy()\ninsurance.columns=['age','bmi','no_of_children','cost']\ninsurance.head()\n\n\n\n\n\n\n\n\nage\nbmi\nno_of_children\ncost\n\n\n\n\n0\n19\n27.900\n0\n16884.92400\n\n\n1\n18\n33.770\n1\n1725.55230\n\n\n2\n28\n33.000\n3\n4449.46200\n\n\n3\n33\n22.705\n0\n21984.47061\n\n\n4\n32\n28.880\n0\n3866.85520\n\n\n\n\n\n\n\n\nsns.lmplot(x='bmi',y='cost',data=insurance)\nplt.xlabel('BMI')\nplt.ylabel('Insurance cost')\nplt.title('Cost Vs BMI');\n\nC:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\n#checking the missing value\ninsurance.apply(pd.isnull).sum()\n\nage               0\nbmi               0\nno_of_children    0\ncost              0\ndtype: int64\n\n\n\n#let's see the correlation plot to identify how related are the features so that we know which features are important\n# correlation plot\ninsurance = insurance.corr()\nsns.heatmap(insurance, cmap = 'summer', annot= True);\n\n\n\n\n\n# so we can see that there is no co-relation between the features,so let's check the pair plots\nsns.pairplot(insurance)\n\nC:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\n# now let's preapare our data to be machine learning ready\n#let's first do one-hot encoding\nfeatures_to_select=['sex','children', 'smoker', 'region']\n\ninsurance_encoded = pd.get_dummies(data = data, prefix = 'encoded', prefix_sep='_',\n               columns = features_to_select,\n               drop_first =True,\n              dtype='int8')\n\n\ninsurance_encoded\n\n\n\n\n\n\n\n\nage\nbmi\ncharges\nencoded_male\nencoded_1\nencoded_2\nencoded_3\nencoded_4\nencoded_5\nencoded_yes\nencoded_northwest\nencoded_southeast\nencoded_southwest\n\n\n\n\n0\n19\n27.900\n16884.92400\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n1\n18\n33.770\n1725.55230\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n28\n33.000\n4449.46200\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n33\n22.705\n21984.47061\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n32\n28.880\n3866.85520\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n10600.54830\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n1334\n18\n31.920\n2205.98080\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1335\n18\n36.850\n1629.83350\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1336\n21\n25.800\n2007.94500\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1337\n61\n29.070\n29141.36030\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n1338 rows × 13 columns\n\n\n\n\ninsurance_encoded['charges'] = np.log(insurance_encoded['charges'])\n\n\n#let's now train our data\n\nfrom sklearn.model_selection import train_test_split\n\nX = insurance_encoded.drop('charges',axis=1) \ny = insurance_encoded['charges']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n\n\n#now let's perform a linear regression model:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Initialize the Linear Regression model\nlinear_model = LinearRegression()\n\n# Fit the model on the training data\nlinear_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred_linear = linear_model.predict(X_test)\n\n# Evaluate the model\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\n\nprint(\"Linear Regression Results:\")\nprint(f\"Mean Squared Error: {mse_linear}\")\nprint(f\"R-squared: {r2_linear}\")\n\nLinear Regression Results:\nMean Squared Error: 0.18729622322981895\nR-squared: 0.7795687545055319\n\n\n\n#I want to check either linear or non-linear model is best for this data so I also want to perform a non-linear regression model:\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Choose the degree of the polynomial\ndegree = 2  # You can experiment with different degrees\n\n# Create a polynomial regression model\npoly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n\n# Fit the model on the training data\npoly_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred_poly = poly_model.predict(X_test)\n\n# Evaluate the model\nmse_poly = mean_squared_error(y_test, y_pred_poly)\nr2_poly = r2_score(y_test, y_pred_poly)\n\nprint(\"\\nPolynomial Regression Results:\")\nprint(f\"Mean Squared Error: {mse_poly}\")\nprint(f\"R-squared: {r2_poly}\")\n\n\nPolynomial Regression Results:\nMean Squared Error: 0.12727884942567\nR-squared: 0.8502039452788253\n\n\n\n#Let's try to visualize both of them:\n\n#For the linear regression data visualizsation:\n\n# Plotting Linear Regression\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=y_test, y=y_pred_linear)\nplt.title(\"Linear Regression: Actual vs Predicted Charges\")\nplt.xlabel(\"Actual Charges\")\nplt.ylabel(\"Predicted Charges\")\n\nText(0, 0.5, 'Predicted Charges')\n\n\n\n\n\n\n# Plotting Polynomial Regression\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=y_test, y=y_pred_poly)\nplt.title(\"Polynomial Regression: Actual vs Predicted Charges\")\nplt.xlabel(\"Actual Charges\")\nplt.ylabel(\"Predicted Charges\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n#In this case, the polynomial regression has a lower MSE and a higher R-squared,\n# which indicates it fits the data better and explains more of the variance.\n\n\n# Accessing coefficients for Polynomial Regression\npoly_coefficients = poly_model.named_steps['linearregression'].coef_\n\n# Creating a DataFrame to display coefficients along with feature names\npoly_coefficients_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Coefficient': poly_coefficients[:len(X_train.columns)]  # Only take coefficients corresponding to original features\n})\n\n# Displaying the coefficients\nprint(poly_coefficients_df)\n\n              Feature   Coefficient\n0                 age  1.023352e-13\n1                 bmi  4.964836e-02\n2        encoded_male  5.215860e-02\n3           encoded_1 -1.934427e-01\n4           encoded_2  1.974225e-01\n5           encoded_3  5.101884e-01\n6           encoded_4  4.195310e-01\n7           encoded_5  3.787017e-01\n8         encoded_yes  3.949445e-01\n9   encoded_northwest  6.499769e-01\n10  encoded_southeast -2.585821e-02\n11  encoded_southwest -4.188016e-02\n\n\n\n# Accessing coefficients for Polynomial Regression\npoly_coefficients = poly_model.named_steps['linearregression'].coef_\n\n# Creating a DataFrame to display coefficients along with feature names\npoly_coefficients_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Coefficient': poly_coefficients[:len(X_train.columns)]  # Only take coefficients corresponding to original features\n})\n\n# Sort coefficients by absolute value for better visualization\npoly_coefficients_df = poly_coefficients_df.reindex(\n    poly_coefficients_df['Coefficient'].abs().sort_values(ascending=False).index\n)\n\n# Create a bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y='Feature', data=poly_coefficients_df, palette='viridis')\nplt.title('Polynomial Regression Coefficients')\nplt.xlabel('Coefficient Value')\nplt.ylabel('Feature')\nplt.show()\n\n\n\n\n#Age: The coefficient is very close to zero, suggesting that age has a minimal impact on the predicted charges.\n#BMI: The coefficient is positive, indicating that an increase in BMI is associated with higher predicted charges.\n#Gender (encoded_male): The coefficient is positive, suggesting that being male is associated with higher predicted charges compared to being female.\n#Children (encoded_1, encoded_2, encoded_3, encoded_4, encoded_5): These coefficients are negative, indicating that having more children is associated with lower predicted charges.\n#Smoker (encoded_yes): The coefficient is positive suggesting that being a smoker is strongly associated with higher predicted charges.\n#Region (encoded_northwest, encoded_southeast, encoded_southwest): These coefficients are positive, with the highest coefficient for ‘encoded_northwest’ indicating that individuals from the northwest region tend to have higher predicted charges."
  },
  {
    "objectID": "posts/Probability and random variable/index.html",
    "href": "posts/Probability and random variable/index.html",
    "title": "Probability and random variable",
    "section": "",
    "text": "This code discusses the probability model for the Titanic disaster.\nImporting the libraries and models:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\n\n# Load the Titanic dataset \ntitanic_df = pd.read_csv('titanic.csv')\n\n\n# Let's assume 'Survived' is the target variable\n# Drop irrelevant columns or handle missing data as needed\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\ntarget = 'Survived'\n\n\n# Convert categorical variables to numerical\ntitanic_df['Sex'] = titanic_df['Sex'].map({'male': 0, 'female': 1})\n\n\n# Handle missing values\ntitanic_df = titanic_df.dropna(subset=features + [target])\n\n\n# Separate features and target variable\nX = titanic_df[features]\ny = titanic_df[target]\n\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Train a Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n\n# Print actual, predicted, and probability values\nresult_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Probability': y_pred_proba})\nprint(result_df)\n\n     Actual  Predicted  Probability\n149       0          0     0.076124\n407       1          0     0.185504\n53        1          1     0.851195\n369       1          1     0.981716\n818       0          0     0.036300\n..      ...        ...          ...\n819       0          0     0.006623\n164       0          0     0.000619\n363       0          0     0.040997\n56        1          1     0.862141\n136       1          1     0.967677\n\n[143 rows x 3 columns]\n\n\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\\n\", classification_rep)\n\nAccuracy: 0.76\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.79      0.83      0.81        87\n           1       0.71      0.66      0.69        56\n\n    accuracy                           0.76       143\n   macro avg       0.75      0.74      0.75       143\nweighted avg       0.76      0.76      0.76       143\n\n\n\n\n# Plot the ROC curve and display the AUC score\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\n\n# Plot ROC curve\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\n\n&lt;matplotlib.legend.Legend at 0x210011aa3d0&gt;\n\n\n\n\n\n\n# Plot precision-recall curve\nplt.subplot(1, 2, 2)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\naverage_precision = average_precision_score(y_test, y_pred_proba)\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Plot probability distribution for positive class\nplt.figure(figsize=(8, 6))\nplt.hist(y_pred_proba[y_test == 1], bins=50, color='blue', alpha=0.7, label='Survived (1)')\nplt.hist(y_pred_proba[y_test == 0], bins=50, color='red', alpha=0.7, label='Not Survived (0)')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Probability Distribution for Positive Class')\nplt.legend(loc='upper right')\nplt.show()"
  }
]